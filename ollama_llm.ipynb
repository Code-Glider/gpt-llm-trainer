{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Fine-tuning with Ollama\n",
    "Using Llama 3.1:8b Instruct model for local fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and activate virtual environment\n",
    "!python -m venv llm_env\n",
    "\n",
    "# Windows activation\n",
    "!call llm_env\\Scripts\\activate.bat\n",
    "# For Unix/Linux/MacOS:\n",
    "# !source llm_env/bin/activate\n",
    "\n",
    "# Verify environment\n",
    "import sys\n",
    "print(f\"Python interpreter: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -q accelerate==0.21.0 \\\n",
    "    peft==0.4.0 \\\n",
    "    bitsandbytes==0.40.2 \\\n",
    "    transformers==4.31.0 \\\n",
    "    trl==0.4.7 \\\n",
    "    ollama-python \\\n",
    "    torch \\\n",
    "    pandas \\\n",
    "    datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"base_model\": \"llama2:3.1-8b-instruct\",\n",
    "    \"fine_tuned_model\": \"llama-3.1-8b-custom\"\n",
    "}\n",
    "\n",
    "prompt = \"A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.\"\n",
    "temperature = 0.4\n",
    "number_of_examples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "import ollama\n",
    "\n",
    "def generate_example(prompt, prev_examples, temperature=0.5):\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"You are generating data which will be used to train a machine learning model.\n",
    "        Generate data samples with prompt/response pairs in this format:\n",
    "        ```\n",
    "        prompt\n",
    "        -----------\n",
    "        $prompt_goes_here\n",
    "        -----------\n",
    "        \n",
    "        response\n",
    "        -----------\n",
    "        $response_goes_here\n",
    "        -----------\n",
    "        ```\n",
    "        Make samples unique and diverse. Here is the model we want to train:\n",
    "        {prompt}\"\"\"\n",
    "    }]\n",
    "\n",
    "    if prev_examples:\n",
    "        if len(prev_examples) > 10:\n",
    "            prev_examples = random.sample(prev_examples, 10)\n",
    "        for example in prev_examples:\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": example\n",
    "            })\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_CONFIG[\"base_model\"],\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Generate examples\n",
    "prev_examples = []\n",
    "for i in range(number_of_examples):\n",
    "    print(f'Generating example {i}')\n",
    "    example = generate_example(prompt, prev_examples, temperature)\n",
    "    prev_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create Datasets\n",
    "prompts = []\n",
    "responses = []\n",
    "\n",
    "for example in prev_examples:\n",
    "    try:\n",
    "        split_example = example.split('-----------')\n",
    "        prompts.append(split_example[1].strip())\n",
    "        responses.append(split_example[3].strip())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'prompt': prompts,\n",
    "    'response': responses\n",
    "})\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Split datasets\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "train_df.to_json('train.jsonl', orient='records', lines=True)\n",
    "test_df.to_json('test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model Training Setup\n",
    "train_dataset = load_dataset('json', data_files='train.jsonl', split=\"train\")\n",
    "valid_dataset = load_dataset('json', data_files='test.jsonl', split=\"train\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    f\"ollama/{MODEL_CONFIG['base_model']}\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"ollama/{MODEL_CONFIG['base_model']}\")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.001,\n",
    "    logging_steps=5\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    max_seq_length=2048\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Inference\n",
    "def generate_response(prompt):\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }]\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=MODEL_CONFIG[\"fine_tuned_model\"],\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=2048\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Test the model\n",
    "test_prompt = \"Write a function that reverses a string.\"\n",
    "result = generate_response(test_prompt)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
