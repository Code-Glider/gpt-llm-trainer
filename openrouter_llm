{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning with OpenRouter and Local Model\n",
    "\n",
    "This notebook uses OpenRouter for data generation and trains a local model.\n",
    "1. Data generation: OpenRouter (various models available)\n",
    "2. Training target: Local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "!python -m venv llm_env\n",
    "\n",
    "# Windows activation\n",
    "!call llm_env\\Scripts\\activate.bat\n",
    "# For Unix/Linux/MacOS:\n",
    "# !source llm_env/bin/activate\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q accelerate==0.21.0 \\\n",
    "    peft==0.4.0 \\\n",
    "    bitsandbytes==0.40.2 \\\n",
    "    transformers==4.31.0 \\\n",
    "    trl==0.4.7 \\\n",
    "    openai \\\n",
    "    torch \\\n",
    "    pandas \\\n",
    "    datasets"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import openai\n",
    "import random\n",
    "\n",
    "# OpenRouter Configuration\n",
    "openai.api_base = \"https://openrouter.ai/api/v1\"\n",
    "openai.api_key = \"YOUR_OPENROUTER_KEY\"  # Replace with your key\n",
    "\n",
    "# Model Configuration\n",
    "OPENROUTER_MODEL = \"anthropic/claude-2\"  # Choose your OpenRouter model\n",
    "LOCAL_MODEL_PATH = \"./local_model\"  # Path to your local model\n",
    "\n",
    "# Training parameters\n",
    "prompt = \"A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.\"\n",
    "temperature = 0.4\n",
    "number_of_examples = 100"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_example(prompt, prev_examples, temperature=0.5):\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"You are generating data which will be used to train a machine learning model.\n",
    "        Generate data samples with prompt/response pairs in this format:\n",
    "        ```\n",
    "        prompt\n",
    "        -----------\n",
    "        $prompt_goes_here\n",
    "        -----------\n",
    "        \n",
    "        response\n",
    "        -----------\n",
    "        $response_goes_here\n",
    "        -----------\n",
    "        ```\n",
    "        Make samples unique and diverse. Here is the model we want to train:\n",
    "        {prompt}\"\"\"\n",
    "    }]\n",
    "\n",
    "    if prev_examples:\n",
    "        if len(prev_examples) > 10:\n",
    "            prev_examples = random.sample(prev_examples, 10)\n",
    "        for example in prev_examples:\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": example\n",
    "            })\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=OPENROUTER_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        headers={\"HTTP-Referer\": \"https://localhost:1234\"},  # Required for OpenRouter\n",
    "    )\n",
    "    return response.choices[0].message['content']\n",
    "\n",
    "# Generate examples\n",
    "prev_examples = []\n",
    "for i in range(number_of_examples):\n",
    "    print(f'Generating example {i}')\n",
    "    example = generate_example(prompt, prev_examples, temperature)\n",
    "    prev_examples.append(example)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_system_message(prompt):\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Generate a concise system prompt for model inference. Format: Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt.strip()\n",
    "    }]\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=OPENROUTER_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        headers={\"HTTP-Referer\": \"https://localhost:1234\"}\n",
    "    )\n",
    "    return response.choices[0].message['content']\n",
    "\n",
    "system_message = generate_system_message(prompt)\n",
    "print(f'System message: {system_message}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create datasets\n",
    "import pandas as pd\n",
    "\n",
    "prompts = []\n",
    "responses = []\n",
    "\n",
    "for example in prev_examples:\n",
    "    try:\n",
    "        split_example = example.split('-----------')\n",
    "        prompts.append(split_example[1].strip())\n",
    "        responses.append(split_example[3].strip())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'prompt': prompts,\n",
    "    'response': responses\n",
    "})\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Split datasets\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "train_df.to_json('train.jsonl', orient='records', lines=True)\n",
    "test_df.to_json('test.jsonl', orient='records', lines=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model Training Setup\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load local model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODEL_PATH,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_dataset('json', data_files='train.jsonl', split=\"train\")\n",
    "valid_dataset = load_dataset('json', data_files='test.jsonl', split=\"train\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    logging_steps=5\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    max_seq_length=2048\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the model\n",
    "from transformers import pipeline\n",
    "\n",
    "def generate_response(prompt):\n",
    "    messages = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{prompt} [/INST]\"\n",
    "    \n",
    "    pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=2048\n",
    "    )\n",
    "    \n",
    "    result = pipe(messages)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Test inference\n",
    "test_prompt = \"Write a function that reverses a string.\"\n",
    "result = generate_response(test_prompt)\n",
    "print(result)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
