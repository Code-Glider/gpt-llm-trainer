{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Fine-tuning: OpenRouter + Local Ollama\n",
    "- Data Generation: OpenRouter (high-quality training data)\n",
    "- Training Target: Local Ollama model (llama2:3.1-8b-instruct)\n",
    "- Hardware: RTX 3090 (24GB VRAM)\n",
    "\n",
    "Prerequisites:\n",
    "1. OpenRouter API key\n",
    "2. Ollama installed\n",
    "3. Run: !ollama pull llama2:3.1-8b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -q accelerate==0.21.0 \\\n",
    "    peft==0.4.0 \\\n",
    "    bitsandbytes==0.40.2 \\\n",
    "    transformers==4.31.0 \\\n",
    "    trl==0.4.7 \\\n",
    "    openai \\\n",
    "    torch \\\n",
    "    pandas \\\n",
    "    datasets"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import openai\n",
    "import random\n",
    "\n",
    "# OpenRouter Configuration\n",
    "openai.api_base = \"https://openrouter.ai/api/v1\"\n",
    "openai.api_key = \"YOUR_OPENROUTER_KEY\"\n",
    "\n",
    "# Model Configuration\n",
    "OPENROUTER_MODEL = \"anthropic/claude-2\"  # For data generation\n",
    "LOCAL_MODEL = \"llama2:3.1-8b-instruct\"   # Local Ollama model\n",
    "\n",
    "# Training parameters\n",
    "prompt = \"A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.\"\n",
    "temperature = 0.4\n",
    "number_of_examples = 100"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_example(prompt, prev_examples, temperature=0.5):\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"You are generating data which will be used to train a machine learning model.\n",
    "        Generate data samples with prompt/response pairs in this format:\n",
    "        ```\n",
    "        prompt\n",
    "        -----------\n",
    "        $prompt_goes_here\n",
    "        -----------\n",
    "        \n",
    "        response\n",
    "        -----------\n",
    "        $response_goes_here\n",
    "        -----------\n",
    "        ```\n",
    "        Make samples unique and diverse. Here is the model we want to train:\n",
    "        {prompt}\"\"\"\n",
    "    }]\n",
    "\n",
    "    if prev_examples:\n",
    "        if len(prev_examples) > 10:\n",
    "            prev_examples = random.sample(prev_examples, 10)\n",
    "        for example in prev_examples:\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": example\n",
    "            })\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=OPENROUTER_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        headers={\"HTTP-Referer\": \"https://localhost:1234\"}\n",
    "    )\n",
    "    return response.choices[0].message['content']\n",
    "\n",
    "# Generate examples\n",
    "prev_examples = []\n",
    "for i in range(number_of_examples):\n",
    "    print(f'Generating example {i}')\n",
    "    example = generate_example(prompt, prev_examples, temperature)\n",
    "    prev_examples.append(example)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_system_message(prompt):\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Generate a concise system prompt for model inference. Format: Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt.strip()\n",
    "    }]\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=OPENROUTER_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        headers={\"HTTP-Referer\": \"https://localhost:1234\"}\n",
    "    )\n",
    "    return response.choices[0].message['content']\n",
    "\n",
    "system_message = generate_system_message(prompt)\n",
    "print(f'System message: {system_message}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "prompts = []\n",
    "responses = []\n",
    "\n",
    "for example in prev_examples:\n",
    "    try:\n",
    "        split_example = example.split('-----------')\n",
    "        prompts.append(split_example[1].strip())\n",
    "        responses.append(split_example[3].strip())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'prompt': prompts,\n",
    "    'response': responses\n",
    "})\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "train_df.to_json('train.jsonl', orient='records', lines=True)\n",
    "test_df.to_json('test.jsonl', orient='records', lines=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# RTX 3090 Optimized Training Setup\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_dataset('json', data_files='train.jsonl', split=\"train\")\n",
    "valid_dataset = load_dataset('json', data_files='test.jsonl', split=\"train\")\n",
    "\n",
    "# Preprocess datasets\n",
    "train_dataset_mapped = train_dataset.map(\n",
    "    lambda examples: {\n",
    "        'text': [\n",
    "            f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n{prompt} [/INST] {response}'\n",
    "            for prompt, response in zip(examples['prompt'], examples['response'])\n",
    "        ]\n",
    "    },\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "valid_dataset_mapped = valid_dataset.map(\n",
    "    lambda examples: {\n",
    "        'text': [\n",
    "            f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n{prompt} [/INST] {response}'\n",
    "            for prompt, response in zip(examples['prompt'], examples['response'])\n",
    "        ]\n",
    "    },\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Load model with RTX 3090 optimizations\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    f\"ollama/{LOCAL_MODEL}\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"ollama/{LOCAL_MODEL}\")\n",
    "\n",
    "# Configure LoRA for 24GB VRAM\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "# Training arguments optimized for RTX 3090\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    logging_steps=5,\n",
    "    save_steps=25,\n",
    "    max_grad_norm=0.3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset_mapped,\n",
    "    eval_dataset=valid_dataset_mapped,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    max_seq_length=2048\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_response(prompt):\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }]\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model='llama2:3.1-8b-instruct-custom',\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=2048\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Test the model\n",
    "test_prompt = \"Write a function that reverses a string.\"\n",
    "result = generate_response(test_prompt)\n",
    "print(result)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
