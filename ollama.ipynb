#  Check environment activation
import sys
print(f"Python interpreter: {sys.executable}")
print(f"Python version: {sys.version}")


# Cell 1: Create and activate environment
"""
## Environment Setup
First, we'll create and activate a Python virtual environment for isolation
"""

# Create virtual environment
!python -m venv llama_env

# Activate virtual environment (for Windows)
!call llama_env\Scripts\activate.bat

# For Unix/Linux/MacOS, use:
# !source llama_env/bin/activate

# Verify Python interpreter
!python --version
!which python  # or !where python on Windows

# Cell 2: Install dependencies
!pip install -q accelerate==0.21.0 \
    peft==0.4.0 \
    bitsandbytes==0.40.2 \
    transformers==4.31.0 \
    trl==0.4.7 \
    ollama-python \
    torch \
    pandas \
    datasets

# Cell 3: Description
"""
## Describe your model -> fine-tuned LLaMA 3.1:8b Instruct
The goal of this notebook is to experiment with fine-tuning Llama 3.1:8b instruct model using Ollama locally.

First, ensure you have:
1. Ollama installed
2. Run: !ollama pull llama2:3.1-8b-instruct

To create your model, just go to the first code cell, and describe the model you want to build in the prompt.
"""

# Cell 2: Setup parameters
prompt = "A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish."
temperature = .4
number_of_examples = 100

# Cell 3: Install dependencies
!pip install -q accelerate peft bitsandbytes transformers trl ollama-python

# Cell 4: Define model parameters
model_name = "llama2:3.1-8b-instruct"
dataset_name = "/content/train.jsonl"
new_model = "llama-3.1-8b-custom"

# LoRA parameters optimized for 8B model
lora_r = 32
lora_alpha = 64
lora_dropout = 0.05

# Training parameters adjusted for 8B model
training_args = {
    "output_dir": "./results",
    "num_train_epochs": 1,
    "per_device_train_batch_size": 2,  # Reduced for 8B model
    "gradient_accumulation_steps": 4,   # Increased for stability
    "learning_rate": 1e-4,             # Adjusted for 8B model
    "weight_decay": 0.001,
    "max_grad_norm": 0.3,
    "logging_steps": 5
}

# Cell 5: Data Generation Function
import ollama
import random

def generate_example(prompt, prev_examples, temperature=0.5):
    messages = [{
        "role": "system",
        "content": f"""You are generating data which will be used to train a machine learning model.
        Generate data samples with prompt/response pairs in this format:
        ```
        prompt
        -----------
        $prompt_goes_here
        -----------
        
        response
        -----------
        $response_goes_here
        -----------
        ```
        Make samples unique and diverse. Here is the model we want to train:
        {prompt}"""
    }]

    if len(prev_examples) > 0:
        if len(prev_examples) > 10:
            prev_examples = random.sample(prev_examples, 10)
        for example in prev_examples:
            messages.append({
                "role": "assistant", 
                "content": example
            })

    response = ollama.chat(
        model=model_name,
        messages=messages,
        temperature=temperature
    )
    return response['message']['content']

# Generate examples
prev_examples = []
for i in range(number_of_examples):
    print(f'Generating example {i}')
    example = generate_example(prompt, prev_examples, temperature)
    prev_examples.append(example)

# Cell 6: Model Training Setup
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig
from trl import SFTTrainer

# Load the model with 8-bit quantization for memory efficiency
model = AutoModelForCausalLM.from_pretrained(
    f"ollama/{model_name}",
    load_in_8bit=True,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(f"ollama/{model_name}")

# Configure LoRA for 8B model
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"]  # Specific to Llama architecture
)

# Initialize trainer with adjusted parameters
trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset_mapped,
    eval_dataset=valid_dataset_mapped,
    peft_config=peft_config,
    tokenizer=tokenizer,
    args=training_args,
    max_seq_length=2048  # Increased for Llama 3.1
)

# Cell 7: Inference
def generate_response(prompt):
    messages = [{
        "role": "system",
        "content": system_message
    },
    {
        "role": "user",
        "content": prompt
    }]
    
    response = ollama.chat(
        model='llama2:3.1-8b-instruct-custom',  # Your fine-tuned model
        messages=messages,
        temperature=0.7,
        max_tokens=2048
    )
    return response['message']['content']
